summary(tweets$Avg)

library(NLP)
library(tm)
#considering tweets with Avg<= -1 as negative
tweets$Avg=ifelse(tweets$Avg<=-1,0,1)
summary(tweets$Avg)
table(tweets$Avg)
corpust=Corpus(VectorSource(tweets$Tweet))
corpust=tm_map(corpust, PlainTextDocument)
corpus = tm_map(corpus, tolower)
corpust = tm_map(corpust, tolower)
corpust = tm_map(corpust, removePunctuation)
corpust = tm_map(corpust, removePunctuation)
corpust = tm_map(corpust, removesWords, c("apple","iphone"),stopwords("english"))
corpust = tm_map(corpust, removeWords, c("apple","iphone"),stopwords("english"))
corpust = tm_map(corpust, removeWords, c("apple","iphone",stopwords("english"))
corpust = tm_map(corpust, removeWords, c("apple","iphone",stopwords("english")))
corpust = tm_map(corpust, removeWords, c("apple","iphone",stopwords("english")))
corpust=tm_map(corpust,stemDocument)
dtmt=DocumentTermMatrix(corpust)
corpust=tm_map(corpust, PlainTextDocument)
dtmt=DocumentTermMatrix(corpust)
sparset=removeSparseTerms(dtmt,0.995)
tweetst1=as.data.frame(as.matrix(sparset))
tweetst1$Avg=tweets$Avg
View(tweetst1)
table(tweetst1$Avg)
library(caTools)
set.seed(13)
split1=sample.split(tweetst1$Avg,SplitRatio = 0.7)
train=subset(tweetst1.split==TRUE)
train=subset(tweetst1.split1==TRUE)
train=subset(tweetst1,split1==TRUE)
test=subset(tweetst1,split1==FALSE)
View(train)
View(train)
View(train)
View(train)
View(test)
modeltree=rpart(Avg~.,data=train,method="class")
predicttree=predict(modeltree,newdata=test)
table(test$Avg,predicttree,type="class")
predicttree=predict(modeltree,newdata=test,type="class")
table(test$Avg,predicttree,type="class")
table(test$Avg,predicttree)
(14+297)/(14+297+41+3)
#using naive bayes
library(e1071)
train$Avg=as.factor(train$Avg)
modelNB=naiveBayes(Avg~.,data=train)
predictNB=predict(modelNB,newdata=test)
table(test$Avg,predictNB)
#neural net
#converting factor to integer
train$Avg=as.integer(train$Avg)
library(neuralnet)
modelNN=neuralnet(Avg~.,data=train,error="sse",hidden=2)
modelNN=neuralnet(Avg~.,data=train,error="ce",hidden=2,linear.output = FALSE)
modelNN=neuralnet(Avg~.,data=train,err.fct ="ce",hidden=2,linear.output = FALSE)
n <- names(train)
f <- as.formula(paste("Avg ~", paste(n[!n %in% "Avg"], collapse = " + ")))
modelNN=neuralnet(f,data=train,err.fct ="ce",hidden=2,linear.output = FALSE)
modelNN$net.result[[1]]
train$Avg=as.integer(train$Avg)
train$Avg=ifelse(train$Avg<2,0,1)
modelNN=neuralnet(f,data=train,err.fct ="ce",hidden=2,linear.output = FALSE)
modelNN$net.result[[1]]
predictNN=modelNN$net.result[[1]]
predictNN=ifelse(predictNN>0.5,1,0)
modelNN=neuralnet(f,data=test,err.fct ="ce",hidden=2,linear.output = FALSE)
predictNN1=modelNN$net.result[[1]]
table(predictNN1,test$Avg)
predictNN1=ifelse(predictNN1>0.5,1,0)
table(predictNN1,test$Avg)
(299+52)/(299+3+1+52)
#now combining the results from random forest, neural net and logistic regression.not using naive bayes because of low accuracy..first we give equal weights to all
result=as.integer(predictLR)+as.integer(predictRF)+as.integer(predictNN)
#factor to integer 0>1 and 1>2...so min value=1 and max=4
result=ifelse(result>2.5,1,0)
table(test$Avg,result)
(31+296)/(31+24+4+296)
#now giving weights as per accuracy
k=as.integer(predictRF)
k=ifelse(k>1,1,0)
resultw=0.4*as.integer(predictNN1)+0.25*as.integer(predictLR)+0.35*k
resultw=ifelse(resultw>0.5,1,0)
table(test$Avg,resultw)
resultw=0.6*as.integer(predictNN1)+0.2*as.integer(predictLR)+0.2*k
table(test$Avg,resultw)
resultw=ifelse(resultw>0.5,1,0)
table(test$Avg,resultw)
(52+299)/(299+4+52)


